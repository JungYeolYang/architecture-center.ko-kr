---
title: 일괄 처리
description: ''
author: zoinerTejada
ms.date: 02/12/2018
ms.openlocfilehash: fe07d4d6501d4778025b75807f4d6be5854c3e09
ms.sourcegitcommit: e7e0e0282fa93f0063da3b57128ade395a9c1ef9
ms.translationtype: HT
ms.contentlocale: ko-KR
ms.lasthandoff: 12/05/2018
ms.locfileid: "52901986"
---
# <a name="batch-processing"></a><span data-ttu-id="e2d2c-102">일괄 처리</span><span class="sxs-lookup"><span data-stu-id="e2d2c-102">Batch processing</span></span>

<span data-ttu-id="e2d2c-103">일반적인 빅 데이터 시나리오는 미사용 데이터의 일괄 처리입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-103">A common big data scenario is batch processing of data at rest.</span></span> <span data-ttu-id="e2d2c-104">이 시나리오에서는 원본 데이터는 원본 응용 프로그램 자체에 의해 또는 오케스트레이션 워크플로에 의해 데이터 저장소에 로드됩니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-104">In this scenario, the source data is loaded into data storage, either by the source application itself or by an orchestration workflow.</span></span> <span data-ttu-id="e2d2c-105">그런 다음 데이터는 병렬 작업에 의해 내부에서 처리됩니다(오케스트레이션 워크플로를 통해서도 처리가 시작될 수 있음).</span><span class="sxs-lookup"><span data-stu-id="e2d2c-105">The data is then processed in-place by a parallelized job, which can also be initiated by the orchestration workflow.</span></span> <span data-ttu-id="e2d2c-106">이러한 처리에는 변환된 결과가 분석 및 보고 구성 요소에 의해 쿼리될 수 있는 분석 데이터 저장소로 로드되기 이전의 여러 반복적인 단계가 포함될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-106">The processing may include multiple iterative steps before the transformed results are loaded into an analytical data store, which can be queried by analytics and reporting components.</span></span>

<span data-ttu-id="e2d2c-107">예를 들어, 웹 서버의 로그가 폴더에 복사된 후 야간에 처리되면서 웹 활동의 일일 보고서가 생성될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-107">For example, the logs from a web server might be copied to a folder and then processed overnight to generate daily reports of web activity.</span></span>

![](./images/batch-pipeline.png)

## <a name="when-to-use-this-solution"></a><span data-ttu-id="e2d2c-108">이 솔루션을 사용해야 하는 경우</span><span class="sxs-lookup"><span data-stu-id="e2d2c-108">When to use this solution</span></span>

<span data-ttu-id="e2d2c-109">일괄 처리는 간단한 데이터 변환에서 보다 완전한 ETL(추출-변환-로드) 파이프라인에 이르는 다양한 시나리오에서 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-109">Batch processing is used in a variety of scenarios, from simple data transformations to a more complete ETL (extract-transform-load) pipeline.</span></span> <span data-ttu-id="e2d2c-110">빅 데이터 컨텍스트에서 일괄 처리는 계산에 상당한 시간이 소요되는 매우 큰 데이터 집합에 작동될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-110">In a big data context, batch processing may operate over very large data sets, where the computation takes significant time.</span></span> <span data-ttu-id="e2d2c-111">(예를 들어, [람다 아키텍처](../big-data/index.md#lambda-architecture)를 참조하세요.) 일괄 처리는 일반적으로 추가적인 대화형 탐색으로 이어지며, 기계 학습을 위한 모델링 지원 데이터를 제공하거나 분석 및 시각화에 최적화된 데이터 저장소로 데이터를 씁니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-111">(For example, see [Lambda architecture](../big-data/index.md#lambda-architecture).) Batch processing typically leads to further interactive exploration, provides the modeling-ready data for machine learning, or writes the data to a data store that is optimized for analytics and visualization.</span></span>

<span data-ttu-id="e2d2c-112">일괄 처리의 한 가지 예는 반구조화된 대규모 플랫 CSV 또는 JSON 파일 집합을 추가 쿼리 준비가 완료된 스키마화 및 구조화 형식으로 변환하는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-112">One example of batch processing is transforming a large set of flat, semi-structured CSV or JSON files into a schematized and structured format that is ready for further querying.</span></span> <span data-ttu-id="e2d2c-113">일반적으로 데이터가 수집에 사용되는 원시 형식(예: CSV)에서 쿼리 성능을 높일 수 있는 이진 형식으로 변환됩니다. 이 방법에서는 데이터를 열 형식으로 저장하고, 종종 데이터에 대한 인덱스 및 인라인 통계를 제공하기 때문에 성능이 향상됩니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-113">Typically the data is converted from the raw formats used for ingestion (such as CSV) into binary formats that are more performant for querying because they store data in a columnar format, and often provide indexes and inline statistics about the data.</span></span>

## <a name="challenges"></a><span data-ttu-id="e2d2c-114">과제</span><span class="sxs-lookup"><span data-stu-id="e2d2c-114">Challenges</span></span>

- <span data-ttu-id="e2d2c-115">**데이터 형식 및 인코딩**</span><span class="sxs-lookup"><span data-stu-id="e2d2c-115">**Data format and encoding**.</span></span> <span data-ttu-id="e2d2c-116">가장 까다로운 디버그 문제 일부는 파일이 예기치 않은 형식이나 인코딩을 사용할 때 나타납니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-116">Some of the most difficult issues to debug happen when files use an unexpected format or encoding.</span></span> <span data-ttu-id="e2d2c-117">예를 들어, 원본 파일은 UTF-16 및 UTF-8 인코딩을 혼합해서 사용하거나 예기치 않은 구분 기호(공백 및 탭)을 포함하거나 예기치 않은 문자를 포함할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-117">For example, source files might use a mix of UTF-16 and UTF-8 encoding, or contain unexpected delimiters (space versus tab), or include unexpected characters.</span></span> <span data-ttu-id="e2d2c-118">또 다른 일반적인 예는 구분 기호로 해석되는 탭, 공백 또는 쉼표를 포함하는 텍스트 필드입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-118">Another common example is text fields that contain tabs, spaces, or commas that are interpreted as delimiters.</span></span> <span data-ttu-id="e2d2c-119">데이터 로드 및 구문 분석 논리는 이러한 문제를 감지하여 처리할 만큼 충분히 유연해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-119">Data loading and parsing logic must be flexible enough to detect and handle these issues.</span></span>

- <span data-ttu-id="e2d2c-120">**시간 조각 조정.**</span><span class="sxs-lookup"><span data-stu-id="e2d2c-120">**Orchestrating time slices.**</span></span> <span data-ttu-id="e2d2c-121">종종 원본 데이터는 연도, 월, 일, 시간 등을 기준으로 구성되는 처리 기간을 반영하는 폴더 계층 구조에 배치됩니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-121">Often source data is placed in a folder hierarchy that reflects processing windows, organized by year, month, day, hour, and so on.</span></span> <span data-ttu-id="e2d2c-122">경우에 따라 데이터 늦게 도착할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-122">In some cases, data may arrive late.</span></span> <span data-ttu-id="e2d2c-123">예를 들어, 웹 서버에서 오류가 발생하면 3월 7일에 대한 로그는 3월 9일까지 처리되기 위한 폴더에 저장되지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-123">For example, suppose that a web server fails, and the logs for March 7th don't end up in the folder for processing until March 9th.</span></span> <span data-ttu-id="e2d2c-124">단지 너무 늦었기 때문에 무시될까요?</span><span class="sxs-lookup"><span data-stu-id="e2d2c-124">Are they just ignored because they're too late?</span></span> <span data-ttu-id="e2d2c-125">다운스트림 처리 논리가 잘못된 순서의 레코드를 처리할 수 있나요?</span><span class="sxs-lookup"><span data-stu-id="e2d2c-125">Can the downstream processing logic handle out-of-order records?</span></span>

## <a name="architecture"></a><span data-ttu-id="e2d2c-126">아키텍처</span><span class="sxs-lookup"><span data-stu-id="e2d2c-126">Architecture</span></span>

<span data-ttu-id="e2d2c-127">일괄 처리 아키텍처는 위 다이어그램에 표시되는 다음 논리적 구성 요소를 갖습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-127">A batch processing architecture has the following logical components, shown in the diagram above.</span></span>

- <span data-ttu-id="e2d2c-128">**데이터 저장소.**</span><span class="sxs-lookup"><span data-stu-id="e2d2c-128">**Data storage.**</span></span> <span data-ttu-id="e2d2c-129">일반적으로 다양한 형식의 대용량 파일의 리포지토리로 사용될 수 있는 분산 파일 저장소입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-129">Typically a distributed file store that can serve as a repository for high volumes of large files in various formats.</span></span> <span data-ttu-id="e2d2c-130">일반적으로 이 종류의 저장소를 데이터 레이크라고도 합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-130">Generically, this kind of store is often referred to as a data lake.</span></span> 

- <span data-ttu-id="e2d2c-131">**일괄 처리.**</span><span class="sxs-lookup"><span data-stu-id="e2d2c-131">**Batch processing.**</span></span> <span data-ttu-id="e2d2c-132">빅 데이터는 일반적으로 대용량이므로 빅 데이터 솔루션은 필터링, 집계 및 그 밖의 분석을 위한 데이터를 준비하기 위해 장기간 실행되는 일괄 처리 작업을 사용하여 데이터 파일을 처리해야 하는 경우가 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-132">The high-volume nature of big data often means that solutions must process data files using long-running batch jobs to filter, aggregate, and otherwise prepare the data for analysis.</span></span> <span data-ttu-id="e2d2c-133">일반적으로 이러한 작업은 원본 파일 읽기 및 처리와 새 파일에 출력 작성으로 이루어집니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-133">Usually these jobs involve reading source files, processing them, and writing the output to new files.</span></span> 

- <span data-ttu-id="e2d2c-134">**분석 데이터 저장소.**</span><span class="sxs-lookup"><span data-stu-id="e2d2c-134">**Analytical data store.**</span></span> <span data-ttu-id="e2d2c-135">대다수의 빅 데이터 솔루션은 분석할 데이터를 준비한 다음, 분석 도구를 사용하여 쿼리할 수 있는 구조화된 형식으로 처리된 데이터를 제공하도록 디자인되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-135">Many big data solutions are designed to prepare data for analysis and then serve the processed data in a structured format that can be queried using analytical tools.</span></span> 

- <span data-ttu-id="e2d2c-136">**분석 및 보고.**</span><span class="sxs-lookup"><span data-stu-id="e2d2c-136">**Analysis and reporting.**</span></span> <span data-ttu-id="e2d2c-137">대부분의 빅 데이터 솔루션의 목표는 분석 및 보고를 통해 데이터에 대한 정보를 제공하는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-137">The goal of most big data solutions is to provide insights into the data through analysis and reporting.</span></span> 

- <span data-ttu-id="e2d2c-138">**오케스트레이션.**</span><span class="sxs-lookup"><span data-stu-id="e2d2c-138">**Orchestration.**</span></span> <span data-ttu-id="e2d2c-139">일괄 처리를 사용할 경우, 일반적으로 데이터를 데이터 저장소, 일괄 처리, 분석 데이터 저장소 및 보고 계층으로 마이그레이션 또는 복사하기 위해 오케스트레이션이 어느 정도 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-139">With batch processing, typically some orchestration is required to migrate or copy the data into your data storage, batch processing, analytical data store, and reporting layers.</span></span>

## <a name="technology-choices"></a><span data-ttu-id="e2d2c-140">기술 선택</span><span class="sxs-lookup"><span data-stu-id="e2d2c-140">Technology choices</span></span>

<span data-ttu-id="e2d2c-141">다음과 같은 기술은 Azure의 일괄 처리 솔루션에 권장됩니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-141">The following technologies are recommended choices for batch processing solutions in Azure.</span></span>

### <a name="data-storage"></a><span data-ttu-id="e2d2c-142">데이터 저장소</span><span class="sxs-lookup"><span data-stu-id="e2d2c-142">Data storage</span></span>

- <span data-ttu-id="e2d2c-143">**Azure Storage Blob 컨테이너**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-143">**Azure Storage Blob Containers**.</span></span> <span data-ttu-id="e2d2c-144">많은 기존 Azure 비즈니스 프로세스가 이미 Azure Blob 저장소를 활용하고 있으므로 빅 데이터 저장소로도 적합합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-144">Many existing Azure business processes already make use of Azure blob storage, making this a good choice for a big data store.</span></span>
- <span data-ttu-id="e2d2c-145">**Azure Data Lake Store**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-145">**Azure Data Lake Store**.</span></span> <span data-ttu-id="e2d2c-146">Azure Data Lake Store는 모든 크기의 파일을 위한 거의 제한 없는 저장소와 포괄적인 보안 옵션을 제공하므로, 이기종 형식의 데이터에 대한 중앙 저장소를 요구하는 대규모 빅 데이터 솔루션에 아주 적합합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-146">Azure Data Lake Store offers virtually unlimited storage for any size of file, and extensive security options, making it a good choice for extremely large-scale big data solutions that require a centralized store for data in heterogeneous formats.</span></span>

<span data-ttu-id="e2d2c-147">자세한 내용은 [데이터 저장소](../technology-choices/data-storage.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-147">For more information, see [Data storage](../technology-choices/data-storage.md).</span></span>

### <a name="batch-processing"></a><span data-ttu-id="e2d2c-148">일괄 처리</span><span class="sxs-lookup"><span data-stu-id="e2d2c-148">Batch processing</span></span>

- <span data-ttu-id="e2d2c-149">**U-SQL**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-149">**U-SQL**.</span></span> <span data-ttu-id="e2d2c-150">U-SQL은 Azure Data Lake Analytics에 사용되는 쿼리 처리 언어입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-150">U-SQL is the query processing language used by Azure Data Lake Analytics.</span></span> <span data-ttu-id="e2d2c-151">이 언어는 SQL의 선언적 특성을 C#의 프로시저 확장성과 결합하고, 병렬처 리를 활용하여 방대한 데이터를 효율적으로 처리할 수 있도록 합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-151">It combines the declarative nature of SQL with the procedural extensibility of C#, and takes advantage of parallelism to enable efficient processing of data at massive scale.</span></span>
- <span data-ttu-id="e2d2c-152">**Hive**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-152">**Hive**.</span></span> <span data-ttu-id="e2d2c-153">Hive는 HDInsight를 비롯한 대부분의 Hadoop 배포에서 지원되는 SQL 유사 언어입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-153">Hive is a SQL-like language that is supported in most Hadoop distributions, including HDInsight.</span></span> <span data-ttu-id="e2d2c-154">Azure Blob 저장소 및 Azure Data Lake Store를 포함하는 모든 HDFS 호환 저장소에서 데이터를 처리하는 데 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-154">It can be used to process data from any HDFS-compatible store, including Azure blob storage and Azure Data Lake Store.</span></span>
- <span data-ttu-id="e2d2c-155">**Pig**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-155">**Pig**.</span></span> <span data-ttu-id="e2d2c-156">Pig는 HDInsight를 비롯한 많은 Hadoop 배포에서 사용되는 선언적 빅 데이터 처리 언어입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-156">Pig is a declarative big data processing language used in many Hadoop distributions, including HDInsight.</span></span> <span data-ttu-id="e2d2c-157">구조화되지 않았거나 반구조화된 데이터를 처리하는 데 특히 유용합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-157">It is particularly useful for processing data that is unstructured or semi-structured.</span></span>
- <span data-ttu-id="e2d2c-158">**Spark**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-158">**Spark**.</span></span> <span data-ttu-id="e2d2c-159">Spark 엔진은 Java, Scala 및 Python 등의 다양한 언어로 작성된 일괄 처리 프로그램을 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-159">The Spark engine supports batch processing programs written in a range of languages, including Java, Scala, and Python.</span></span> <span data-ttu-id="e2d2c-160">Spark는 분산 아키텍처를 사용하여 여러 작업자 노드에서 동시에 데이터를 처리합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-160">Spark uses a distributed architecture to process data in parallel across multiple worker nodes.</span></span>

<span data-ttu-id="e2d2c-161">자세한 내용은 [일괄 처리](../technology-choices/batch-processing.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-161">For more information, see [Batch processing](../technology-choices/batch-processing.md).</span></span>

### <a name="analytical-data-store"></a><span data-ttu-id="e2d2c-162">분석 데이터 저장소</span><span class="sxs-lookup"><span data-stu-id="e2d2c-162">Analytical data store</span></span>

- <span data-ttu-id="e2d2c-163">**SQL Data Warehouse**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-163">**SQL Data Warehouse**.</span></span> <span data-ttu-id="e2d2c-164">Azure SQL Data Warehouse는 SQL Server 데이터베이스 기술을 기준으로 하며, 대규모 데이터 웨어하우징 작업을 지원하도록 최적화된 관리되는 서비스입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-164">Azure SQL Data Warehouse is a managed service based on SQL Server database technologies and optimized to support large-scale data warehousing workloads.</span></span>
- <span data-ttu-id="e2d2c-165">**Spark SQL**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-165">**Spark SQL**.</span></span> <span data-ttu-id="e2d2c-166">Spark SQL은 SQL 구문을 사용하여 쿼리할 수 있는 데이터프레임 및 테이블을 만들 수 있도록 지원하는 Spark 기반 API입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-166">Spark SQL is an API built on Spark that supports the creation of dataframes and tables that can be queried using SQL syntax.</span></span>
- <span data-ttu-id="e2d2c-167">**HBase**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-167">**HBase**.</span></span> <span data-ttu-id="e2d2c-168">HBase는 구조화 및 반구조화 데이터를 쿼리하기 위한 유연한 고성능 옵션을 제공하는 낮은 대기 시간의 NoSQL 저장소입니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-168">HBase is a low-latency NoSQL store that offers a high-performance, flexible option for querying structured and semi-structured data.</span></span>
- <span data-ttu-id="e2d2c-169">**Hive**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-169">**Hive**.</span></span> <span data-ttu-id="e2d2c-170">일괄 처리에 유용하다는 것 외에도, Hive는 전형적인 관계형 데이터베이스 관리 시스템과 개념적으로 유사한 데이터베이스 아키텍처를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-170">In addition to being useful for batch processing, Hive offers a database architecture that is conceptually similar to that of a typical relational database management system.</span></span> <span data-ttu-id="e2d2c-171">Tez 엔진 및 Stinger 이니셔티브와 같은 혁신을 통해 Hive 쿼리 성능을 개선했으므로 일부 시나리오에서는 Hive 테이블을 분석 쿼리의 원본으로 효과적으로 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-171">Improvements in Hive query performance through innovations like the Tez engine and Stinger initiative mean that Hive tables can be used effectively as sources for analytical queries in some scenarios.</span></span>

<span data-ttu-id="e2d2c-172">자세한 내용은 [분석 데이터 저장소](../technology-choices/analytical-data-stores.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-172">For more information, see [Analytical data stores](../technology-choices/analytical-data-stores.md).</span></span>

### <a name="analytics-and-reporting"></a><span data-ttu-id="e2d2c-173">분석 및 보고</span><span class="sxs-lookup"><span data-stu-id="e2d2c-173">Analytics and reporting</span></span>

- <span data-ttu-id="e2d2c-174">**Azure Analysis Services**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-174">**Azure Analysis Services**.</span></span> <span data-ttu-id="e2d2c-175">많은 빅 데이터 솔루션에서는 보고서, 대시보드 및 대화형 "조각화 및 분석" 방식의 분석의 기준이 될 수 있는 중앙 집중식 OLAP(온라인 분석 처리) 데이터 모델(큐브라고도 함)을 포함하여 기존 엔터프라이즈 비즈니스 인텔리전스 아키텍처를 에뮬레이트합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-175">Many big data solutions emulate traditional enterprise business intelligence architectures by including a centralized online analytical processing (OLAP) data model (often referred to as a cube) on which reports, dashboards, and interactive “slice and dice” analysis can be based.</span></span> <span data-ttu-id="e2d2c-176">Azure Analysis Services는 이 요구 사항을 충족하도록 테이블 형식 모델 만들기를 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-176">Azure Analysis Services supports the creation of tabular models to meet this need.</span></span>
- <span data-ttu-id="e2d2c-177">**Power BI**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-177">**Power BI**.</span></span> <span data-ttu-id="e2d2c-178">Power BI는 데이터 분석가가 OLAP 모델에서 또는 분석 데이터 저장소에서 직접, 데이터 모델을 기준으로 대화형 데이터 시각화를 만들 수 있도록 합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-178">Power BI enables data analysts to create interactive data visualizations based on data models in an OLAP model or directly from an analytical data store.</span></span>
- <span data-ttu-id="e2d2c-179">**Microsoft Excel**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-179">**Microsoft Excel**.</span></span> <span data-ttu-id="e2d2c-180">Microsoft Excel은 전 세계적으로 가장 널리 사용되는 소프트웨어 응용 프로그램 중 하나로, 다양한 데이터 분석 및 시각화 기능을 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-180">Microsoft Excel is one of the most widely used software applications in the world, and offers a wealth of data analysis and visualization capabilities.</span></span> <span data-ttu-id="e2d2c-181">데이터 분석가는 Excel을 사용하여 분석 데이터 저장소에서 문서 데이터 모델을 작성하거나 OLAP 데이터 모델의 데이터를 대화형 피벗 테이블 및 차트로 검색할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-181">Data analysts can use Excel to build document data models from analytical data stores, or to retrieve data from OLAP data models into interactive PivotTables and charts.</span></span>

<span data-ttu-id="e2d2c-182">자세한 내용은 [분석 및 보고](../technology-choices/analysis-visualizations-reporting.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-182">For more information, see [Analytics and reporting](../technology-choices/analysis-visualizations-reporting.md).</span></span>

### <a name="orchestration"></a><span data-ttu-id="e2d2c-183">오케스트레이션</span><span class="sxs-lookup"><span data-stu-id="e2d2c-183">Orchestration</span></span>

- <span data-ttu-id="e2d2c-184">**Azure Data Factory**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-184">**Azure Data Factory**.</span></span> <span data-ttu-id="e2d2c-185">Azure Data Factory 파이프라인은 반복되는 기간 동안 예약된 작업 시퀀스를 정의하는 데 사용될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-185">Azure Data Factory pipelines can be used to define a sequence of activities, scheduled for recurring temporal windows.</span></span> <span data-ttu-id="e2d2c-186">이러한 작업은 요청 시 HDInsight 클러스터의 Hive, Pig, MapReduce 또는 Spark 작업, Azure Date Lake Analytics의 U-SQL 작업, Azure SQL Data Warehouse 또는 Azure SQL Database의 저장 프로시저 뿐만 아니라 데이터 복사 작업을 시작할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-186">These activities can initiate data copy operations as well as Hive, Pig, MapReduce, or Spark jobs in on-demand HDInsight clusters; U-SQL jobs in Azure Date Lake Analytics; and stored procedures in Azure SQL Data Warehouse or Azure SQL Database.</span></span>
- <span data-ttu-id="e2d2c-187">**Oozie** 및 **Sqoop**.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-187">**Oozie** and **Sqoop**.</span></span> <span data-ttu-id="e2d2c-188">Oozie는 Apache Hadoop 에코시스템용 작업 자동화 엔진으로, 데이터를 처리하기 위한 Hive, Pig 및 MapReduce 작업, HDFS 데이터베이스와 SQL 데이터베이스 간에 데이터를 복사하는 Sqoop 작업 뿐만 아니라 데이터 복사 작업을 시작하는 데 사용될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-188">Oozie is a job automation engine for the Apache Hadoop ecosystem and can be used to initiate data copy operations as well as Hive, Pig, and MapReduce jobs to process data and Sqoop jobs to copy data between HDFS and SQL databases.</span></span>

<span data-ttu-id="e2d2c-189">자세한 내용은 [파이프라인 오케스트레이션](../technology-choices/pipeline-orchestration-data-movement.md)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="e2d2c-189">For more information, see [Pipeline orchestration](../technology-choices/pipeline-orchestration-data-movement.md)</span></span>
